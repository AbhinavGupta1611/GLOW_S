{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ab9cf2-4f84-4afd-8e7d-605e5dacffff",
   "metadata": {},
   "source": [
    "## Sinusoidal curve fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d0af66-8ee6-4f75-8141-f2f653daef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "from dbfread import DBF\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from lmfit import Model\n",
    "from lmfit import minimize,Parameters,Parameter,report_fit\n",
    "\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0d0713-c287-4527-94fb-363473d9e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteredComidTo60pcMedian(one_comid):\n",
    "    one_comid_copy = one_comid.copy()\n",
    "    min_data_taken_from_high_freq_stn = 0.6\n",
    "\n",
    "    # Make proper date format\n",
    "    one_comid_copy['date'] = pd.to_datetime(one_comid_copy['date'])\n",
    "    base_date = pd.to_datetime('2000-01-01')\n",
    "    one_comid_copy['days2000'] = (one_comid_copy['date'] - base_date).dt.days\n",
    "    one_comid_copy.drop('date', axis=1, inplace=True)\n",
    "    \n",
    "    # Limit the data by taking 60% data from top n frequent COMID\n",
    "    river_id_counts = one_comid_copy['riverID'].value_counts()\n",
    "    cumulative_sum = river_id_counts.cumsum()\n",
    "    total_sum = river_id_counts.sum()\n",
    "    threshold = min_data_taken_from_high_freq_stn * total_sum\n",
    "    filtered_data = one_comid_copy[one_comid_copy['riverID'].isin(river_id_counts.index[0:cumulative_sum[cumulative_sum < threshold].count() + 1])]\n",
    "    \n",
    "    filtered_data['width_norm'] = filtered_data['width'] * filtered_data['width'].mean() / filtered_data.groupby('riverID')['width'].transform('mean')\n",
    "    \n",
    "    # Limit the data to 2% and 98% quantile width_norm\n",
    "    quantile_bot = filtered_data['width_norm'].quantile(0.02)\n",
    "    quantile_top = filtered_data['width_norm'].quantile(0.98)\n",
    "    filtered_data = filtered_data[(filtered_data['width_norm'] > quantile_bot) & (filtered_data['width_norm'] < quantile_top)]\n",
    "    \n",
    "    # Median data computation\n",
    "    median_days2000 = filtered_data.groupby('days2000')['width_norm'].median()\n",
    "    return median_days2000\n",
    "\n",
    "# Standard Error of Means\n",
    "def calculate_sem_from_residuals(residuals):\n",
    "    std_dev = np.std(residuals)\n",
    "    n = len(residuals)\n",
    "    sem = std_dev / np.sqrt(n)\n",
    "    return sem\n",
    "\n",
    "# Root Relative Squared Error (RRSE)\n",
    "def calculate_RRSE(original_data, projected_data):\n",
    "    mean_original = np.mean(original_data)\n",
    "    numerator = np.sum((projected_data - original_data) ** 2)\n",
    "    denominator = np.sum((mean_original - original_data) ** 2)\n",
    "    return np.sqrt(numerator / denominator)\n",
    "\n",
    "def perform_kuipers_test(data1, data2):\n",
    "    # Perform the Kuipers test for two datasets\n",
    "    ks_statistic = ks_2samp(data1, data2).statistic\n",
    "    # A great fit has a low KS statistic, indicating similarity between the two datasets.\n",
    "    # A bad fit has a high KS statistic, indicating dissimilarity between the two datasets.\n",
    "    return ks_statistic\n",
    "\n",
    "# Relative Absolute Error (RAE)\n",
    "def calculate_RAE(original_data, projected_data):\n",
    "    mean_original = np.mean(original_data)\n",
    "    numerator = np.sum(np.abs(projected_data - original_data))\n",
    "    denominator = np.sum(np.abs(mean_original - original_data))\n",
    "    return numerator / denominator\n",
    "\n",
    "# Normalized Root Mean Square Error (NRMSE)\n",
    "def calculate_NRMSE(original_data, projected_data):\n",
    "    rmse = np.sqrt(np.mean((projected_data - original_data) ** 2))\n",
    "    data_range = np.max(original_data) - np.min(original_data)\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse\n",
    "\n",
    "# Define the sine function with a custom time period\n",
    "def sine_curve_one(x, amplitude, intercept, phase):\n",
    "    time_period = 365.25\n",
    "    return np.abs(float(amplitude)) * np.sin((2 * np.pi * x / time_period) + (2 * np.pi * phase / time_period)) + intercept\n",
    "\n",
    "def calculate_cv_iqr(data):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    cv = (std_dev / mean) * 100\n",
    "\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    return cv, iqr, len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27765f1e-0c5f-487d-9914-d818db49b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_COMID(thatCOMID, thatCOMID_sampling, COMID_dataCount):\n",
    "    mean_months = thatCOMID_sampling.groupby('month')['width'].mean()\n",
    "    unique_months = len(mean_months)\n",
    "    cv_month, iqr_month, count_month = calculate_cv_iqr(mean_months.values)\n",
    "\n",
    "    median_days2000 = filteredComidTo60pcMedian(thatCOMID_sampling)\n",
    "    residuals_flat = median_days2000 - median_days2000.mean()\n",
    "    COMID_dataCount[thatCOMID] =  len(median_days2000)\n",
    "\n",
    "    if len(median_days2000) > 60:\n",
    "        standard_error = calculate_sem_from_residuals(residuals_flat)\n",
    "        cv = stats.variation(median_days2000.values) * 100\n",
    "\n",
    "        dates = median_days2000.index\n",
    "        widths = median_days2000.values\n",
    "        stddev = np.std(widths)\n",
    "\n",
    "        x = np.asarray(dates)\n",
    "        y = np.asarray(widths)\n",
    "\n",
    "        # Define multiple sets of parameters with different phase guesses\n",
    "        params0 = Parameters()\n",
    "        params0.add('amplitude', value=stddev * 5, min=0)\n",
    "        params0.add('intercept', value=np.median(y) / 2, min=0)\n",
    "        params0.add('phase', value=0, min=-365.25 * 2, max=365.25 * 3)\n",
    "\n",
    "        params90 = Parameters()\n",
    "        params90.add('amplitude', value=stddev * 5, min=0)\n",
    "        params90.add('intercept', value=np.median(y) / 2, min=0)\n",
    "        params90.add('phase', value=90, min=-365.25 * 2, max=365.25 * 3)\n",
    "\n",
    "        params180 = Parameters()\n",
    "        params180.add('amplitude', value=stddev * 5, min=0)\n",
    "        params180.add('intercept', value=np.median(y) / 2, min=0)\n",
    "        params180.add('phase', value=180, min=-365.25 * 2, max=365.25 * 3)\n",
    "\n",
    "        params270 = Parameters()\n",
    "        params270.add('amplitude', value=stddev * 5, min=0)\n",
    "        params270.add('intercept', value=np.median(y) / 2, min=0)\n",
    "        params270.add('phase', value=270, min=-365.25 * 2, max=365.25 * 3)\n",
    "\n",
    "        # Create models with different phase guesses\n",
    "        dmodel0 = Model(sine_curve_one)\n",
    "        result0 = dmodel0.fit(y, params0, x=x)\n",
    "        \n",
    "        dmodel90 = Model(sine_curve_one)\n",
    "        result90 = dmodel90.fit(y, params90, x=x)\n",
    "        \n",
    "        dmodel180 = Model(sine_curve_one)\n",
    "        result180 = dmodel180.fit(y, params180, x=x)\n",
    "\n",
    "        dmodel270 = Model(sine_curve_one)\n",
    "        result270 = dmodel270.fit(y, params270, x=x)\n",
    "        \n",
    "        # Calculate the sum of squared residuals for each fit\n",
    "        ssr0 = np.sum(result0.residual ** 2)\n",
    "        ssr90 = np.sum(result90.residual ** 2)\n",
    "        ssr180 = np.sum(result180.residual ** 2)\n",
    "        ssr270 = np.sum(result270.residual ** 2)\n",
    "\n",
    "        # Choose the best fit based on the least sum of squares (LSS)\n",
    "        best_result = min((result0, result90, result180, result270), key=lambda result: np.sum(result.residual ** 2))\n",
    "\n",
    "        amp = best_result.params['amplitude'].value\n",
    "        inc = best_result.params['intercept'].value\n",
    "        pek = best_result.params['phase'].value\n",
    "\n",
    "        params = (amp, inc, pek)\n",
    "        sine_curve_one_gen_data = sine_curve_one(dates, *params)\n",
    "        residuals_curve = median_days2000.values - sine_curve_one_gen_data\n",
    "        standard_error_sine = calculate_sem_from_residuals(residuals_curve)\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(median_days2000.index, median_days2000.values)\n",
    "\n",
    "        iqr = np.percentile(widths, 75) - np.percentile(widths, 25)\n",
    "        median_widths = np.median(widths)\n",
    "\n",
    "        ks_stat2 = perform_kuipers_test(widths, sine_curve_one_gen_data)\n",
    "        rrse = calculate_RRSE(widths, sine_curve_one_gen_data)\n",
    "        rae = calculate_RAE(widths, sine_curve_one_gen_data)\n",
    "        nrmse = calculate_NRMSE(widths, sine_curve_one_gen_data)\n",
    "\n",
    "        return [thatCOMID, amp, inc, pek, cv, slope, intercept, r_value ** 2, p_value, standard_error, standard_error_sine, ks_stat2, rae, rrse, nrmse, stddev, iqr, median_widths, cv_month, iqr_month, count_month]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def worker(args):\n",
    "    return process_COMID(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb752b2c-a4e7-4739-9693-018fe0bf7888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def send_ntfy_notification(title, message, priority=\"urgent\", tags=\"bell,fire\"):\n",
    "    url = \"https://ntfy.sh/river_width_res\"\n",
    "    headers = {\n",
    "        \"Title\": title,\n",
    "        \"Priority\": priority,\n",
    "        \"Tags\": tags\n",
    "    }\n",
    "    response = requests.post(url, data=message, headers=headers)\n",
    "    return response\n",
    "\n",
    "# Example usage after a cell completes\n",
    "send_ntfy_notification(\n",
    "    title=\"Notebook Alert: Cell Execution Complete\",\n",
    "    message=\"Your cell has successfully completed execution!\",\n",
    "    priority=\"urgent\",\n",
    "    tags=\"bell,fire\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfeb922-f7c6-4795-b7fc-81e244919551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GLOW-S width with snow affected values removed \n",
    "output_path = '/N/lustre/project/proj-212/abhinav/River_Width_analysis/RiverWidthAnalysis/ClimateRegionDivision/HUC_Parquet/allRegion_large_T273check_over60.parquet'\n",
    "final_large_data_warm_over60 = pd.read_parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f31a2c-e848-4d6a-aa31-160417843fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>riverID</th>\n",
       "      <th>date</th>\n",
       "      <th>width</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>sceneID_unique</th>\n",
       "      <th>COMID</th>\n",
       "      <th>month</th>\n",
       "      <th>date_YMD</th>\n",
       "      <th>hot_enough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R12084791XS2389944</td>\n",
       "      <td>2019-01-12 10:09:57</td>\n",
       "      <td>8.448136</td>\n",
       "      <td>-28.952676</td>\n",
       "      <td>27.725176</td>\n",
       "      <td>95294</td>\n",
       "      <td>12084791</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R12084791XS2389945</td>\n",
       "      <td>2019-01-12 10:09:57</td>\n",
       "      <td>1.586848</td>\n",
       "      <td>-28.952500</td>\n",
       "      <td>27.724663</td>\n",
       "      <td>95294</td>\n",
       "      <td>12084791</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R12084791XS2389944</td>\n",
       "      <td>2019-01-12 10:09:57</td>\n",
       "      <td>10.141748</td>\n",
       "      <td>-28.952676</td>\n",
       "      <td>27.725176</td>\n",
       "      <td>95295</td>\n",
       "      <td>12084791</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R12084791XS2389945</td>\n",
       "      <td>2019-01-12 10:09:57</td>\n",
       "      <td>1.586848</td>\n",
       "      <td>-28.952500</td>\n",
       "      <td>27.724663</td>\n",
       "      <td>95295</td>\n",
       "      <td>12084791</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R12084791XS2389910</td>\n",
       "      <td>2019-02-06 10:50:10</td>\n",
       "      <td>21.861701</td>\n",
       "      <td>-28.965000</td>\n",
       "      <td>27.723562</td>\n",
       "      <td>95361</td>\n",
       "      <td>12084791</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400715000</th>\n",
       "      <td>R78027927XS0147569</td>\n",
       "      <td>2022-12-06 21:50:59</td>\n",
       "      <td>22.856135</td>\n",
       "      <td>42.185966</td>\n",
       "      <td>-124.134868</td>\n",
       "      <td>33477</td>\n",
       "      <td>78027927</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400715001</th>\n",
       "      <td>R78027927XS0147569</td>\n",
       "      <td>2022-12-13 22:25:00</td>\n",
       "      <td>82.885030</td>\n",
       "      <td>42.185966</td>\n",
       "      <td>-124.134868</td>\n",
       "      <td>33484</td>\n",
       "      <td>78027927</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400715002</th>\n",
       "      <td>R78027927XS0147570</td>\n",
       "      <td>2022-12-13 22:25:00</td>\n",
       "      <td>34.744213</td>\n",
       "      <td>42.186366</td>\n",
       "      <td>-124.134467</td>\n",
       "      <td>33484</td>\n",
       "      <td>78027927</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400715003</th>\n",
       "      <td>R78027927XS0147569</td>\n",
       "      <td>2022-12-16 22:22:55</td>\n",
       "      <td>58.222716</td>\n",
       "      <td>42.185966</td>\n",
       "      <td>-124.134868</td>\n",
       "      <td>33498</td>\n",
       "      <td>78027927</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400715004</th>\n",
       "      <td>R78027927XS0147570</td>\n",
       "      <td>2022-12-16 22:22:55</td>\n",
       "      <td>6.303858</td>\n",
       "      <td>42.186366</td>\n",
       "      <td>-124.134467</td>\n",
       "      <td>33498</td>\n",
       "      <td>78027927</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400715005 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       riverID                date      width        lat  \\\n",
       "0           R12084791XS2389944 2019-01-12 10:09:57   8.448136 -28.952676   \n",
       "1           R12084791XS2389945 2019-01-12 10:09:57   1.586848 -28.952500   \n",
       "2           R12084791XS2389944 2019-01-12 10:09:57  10.141748 -28.952676   \n",
       "3           R12084791XS2389945 2019-01-12 10:09:57   1.586848 -28.952500   \n",
       "4           R12084791XS2389910 2019-02-06 10:50:10  21.861701 -28.965000   \n",
       "...                        ...                 ...        ...        ...   \n",
       "1400715000  R78027927XS0147569 2022-12-06 21:50:59  22.856135  42.185966   \n",
       "1400715001  R78027927XS0147569 2022-12-13 22:25:00  82.885030  42.185966   \n",
       "1400715002  R78027927XS0147570 2022-12-13 22:25:00  34.744213  42.186366   \n",
       "1400715003  R78027927XS0147569 2022-12-16 22:22:55  58.222716  42.185966   \n",
       "1400715004  R78027927XS0147570 2022-12-16 22:22:55   6.303858  42.186366   \n",
       "\n",
       "                   lon  sceneID_unique     COMID  month    date_YMD  \\\n",
       "0            27.725176           95294  12084791      1  2019-01-12   \n",
       "1            27.724663           95294  12084791      1  2019-01-12   \n",
       "2            27.725176           95295  12084791      1  2019-01-12   \n",
       "3            27.724663           95295  12084791      1  2019-01-12   \n",
       "4            27.723562           95361  12084791      2  2019-02-06   \n",
       "...                ...             ...       ...    ...         ...   \n",
       "1400715000 -124.134868           33477  78027927     12  2022-12-06   \n",
       "1400715001 -124.134868           33484  78027927     12  2022-12-13   \n",
       "1400715002 -124.134467           33484  78027927     12  2022-12-13   \n",
       "1400715003 -124.134868           33498  78027927     12  2022-12-16   \n",
       "1400715004 -124.134467           33498  78027927     12  2022-12-16   \n",
       "\n",
       "            hot_enough  \n",
       "0                  1.0  \n",
       "1                  1.0  \n",
       "2                  1.0  \n",
       "3                  1.0  \n",
       "4                  1.0  \n",
       "...                ...  \n",
       "1400715000         1.0  \n",
       "1400715001         1.0  \n",
       "1400715002         1.0  \n",
       "1400715003         1.0  \n",
       "1400715004         1.0  \n",
       "\n",
       "[1400715005 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_large_data_warm_over60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f073c6-43d0-4bb5-b6bc-99a780fd24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the rows with width less than 10m\n",
    "final_large_data_warm_over60 = final_large_data_warm_over60[final_large_data_warm_over60['width']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29b5027-851b-4e45-b83c-3d71bbd8b101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23251578,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_large_data_warm_over60['riverID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7653867-359c-4e21-908b-9409a3f60701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 476232/476232 [21:20<00:00, 371.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "index = 0\n",
    "\n",
    "\n",
    "COMID_dataCount = {}\n",
    "COMID_dataCollection = [['COMID', 'amplitude', 'intercept', 'phase', 'CV_flat', 'slope', 'intercept_c', 'R2', 'p_value', 'SE_flat', 'SE_sine', 'ks_stat', 'rae', 'rrse', 'nrmse', 'stddev', 'IQR', 'Med_width', 'CV_month', 'IQR_month', 'Count_month']]\n",
    "\n",
    "jobs = [(thatCOMID, thatCOMID_sampling, COMID_dataCount) for thatCOMID, thatCOMID_sampling in final_large_data_warm_over60.groupby('COMID')]\n",
    "num_processes = 8\n",
    "\n",
    "with multiprocessing.Pool(num_processes) as pool:results = list(tqdm(pool.imap(worker, jobs), total=len(jobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6c6dc0-a092-43bd-9644-1fa0edc43ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    if result:\n",
    "        COMID_dataCollection.append(result)\n",
    "\n",
    "final_large_data_warm_over60_sine_fitting = pd.DataFrame(COMID_dataCollection[1:], columns=COMID_dataCollection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0adae1d3-eb7e-46d9-86b8-4f59cc3a8bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342367, 21)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_large_data_warm_over60_sine_fitting.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394efb1f-9da2-436d-bb7e-a0b7412ce33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /N/lustre/project/proj-212/abhinav/River_Width_analysis/RiverWidthAnalysis/NCDF_out/sine_peak_newresult_below_10m_removed.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = '/N/lustre/project/proj-212/abhinav/River_Width_analysis/RiverWidthAnalysis/NCDF_out/sine_peak_newresult_below_10m_removed.csv'\n",
    "final_large_data_warm_over60_sine_fitting.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20198e-2af4-4fe2-a46c-bc4b88168175",
   "metadata": {},
   "source": [
    "## IQR analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a46dee3e-6b5d-445b-badc-566c07d880ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the function that processes each COMID\n",
    "def process_comid(thatCOMID, thatCOMID_sampling):\n",
    "    median_days2000 = filteredComidTo60pcMedian(thatCOMID_sampling)\n",
    "    dates = median_days2000.index\n",
    "    widths = median_days2000.values\n",
    "\n",
    "    if median_days2000.shape[0] == 0:\n",
    "        return [thatCOMID, -1, -1, thatCOMID_sampling.shape[0], len(thatCOMID_sampling['date'].unique())]\n",
    "\n",
    "    # Calculate IQR and median\n",
    "    iqr = np.percentile(widths, 75) - np.percentile(widths, 25)\n",
    "    median_widths = np.median(widths)\n",
    "    count = thatCOMID_sampling.shape[0]\n",
    "    days = len(thatCOMID_sampling['date'].unique())\n",
    "\n",
    "    # Return the results for this COMID\n",
    "    return [thatCOMID, iqr, median_widths, count, days]\n",
    "\n",
    "# Define a helper function to pass to multiprocessing.Pool\n",
    "def worker(job):\n",
    "    return process_comid(*job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d48537b3-6e9a-4170-a841-b9ced6781741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 476232/476232 [08:50<00:00, 897.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the job list (pairs of thatCOMID and thatCOMID_sampling)\n",
    "jobs = [(thatCOMID, thatCOMID_sampling) for thatCOMID, thatCOMID_sampling in final_large_data_warm_over60.groupby('COMID')]\n",
    "\n",
    "# Use multiprocessing with 6 cores and tqdm for progress\n",
    "num_cores = 8\n",
    "with multiprocessing.Pool(num_cores) as pool:\n",
    "    results = list(tqdm(pool.imap(worker, jobs), total=len(jobs)))\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['COMID', 'IQR', 'Median_Width', 'Count', 'Days'])\n",
    "\n",
    "# Add a new column for IQRc\n",
    "results_df['IQRc'] = results_df['IQR'] / results_df['Median_Width']\n",
    "\n",
    "# Ensure Days and Count columns are integers\n",
    "results_df['Days'] = results_df['Days'].astype(int)\n",
    "results_df['Count'] = results_df['Count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1c27cfe-5d46-4d40-aea9-177eb4d0b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'NCDF_out/IQR_newresult_below_10m_removed.csv'\n",
    "results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f04ff-38e0-4454-b0fd-2bbe6f304a35",
   "metadata": {},
   "source": [
    "## Kruskal-Wallis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982aef9f-2252-4321-ac1d-876310d89684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 476232/476232 [1:20:25<00:00, 98.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476232\n",
      "Number of lakes are  165\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "all_params = {}\n",
    "\n",
    "min_data_taken_from_high_freq_stn = 0.6\n",
    "base_date = pd.to_datetime('2000-01-01')\n",
    "final_large_data_warm_over60['days2000'] = (final_large_data_warm_over60['date'] - base_date).dt.days\n",
    "final_large_data_warm_over60['days2000'] = pd.to_datetime(final_large_data_warm_over60['date'].dt.strftime('%Y-%m-%d'))\n",
    "latlon_comid = final_large_data_warm_over60.groupby('COMID').first().reset_index()\n",
    "\n",
    "\n",
    "min_days_data = 20\n",
    "min_data_taken_from_high_freq_stn = 0.6 #60% of the data is held\n",
    "\n",
    "df_summary = pd.DataFrame(columns=['COMID', 'seasonality'])\n",
    "\n",
    "count = 0\n",
    "count_lake = 0\n",
    "for thatCOMID, thatCOMID_sampling in tqdm(final_large_data_warm_over60.groupby(['COMID'])):\n",
    "    count += 1\n",
    "    \n",
    "    river_id_counts = thatCOMID_sampling['riverID'].value_counts()\n",
    "    cumulative_sum = river_id_counts.cumsum()\n",
    "    total_sum = river_id_counts.sum()\n",
    "    threshold = min_data_taken_from_high_freq_stn * total_sum\n",
    "    filtered_data = thatCOMID_sampling[thatCOMID_sampling['riverID'].isin(river_id_counts.index[0:cumulative_sum[cumulative_sum < threshold].count() + 1])]\n",
    "    \n",
    "    filtered_data['width_normalized'] = filtered_data['width'] / filtered_data.groupby('riverID')['width'].transform('mean')\n",
    "    median_days2000 = filtered_data.groupby('days2000')['width_normalized'].median()\n",
    "    \n",
    "    median = median_days2000.median()\n",
    "    iqr = median_days2000.quantile(0.75) - median_days2000.quantile(0.25)\n",
    "    \n",
    "    threshold = 1.5 * iqr\n",
    "    \n",
    "    lower_bound = median - threshold\n",
    "    upper_bound = median + threshold\n",
    "    \n",
    "    median_days2000 = median_days2000[(median_days2000 >= lower_bound) & (median_days2000 <= upper_bound)]\n",
    "    \n",
    "    if median_days2000.shape[0] < min_days_data:\n",
    "        df_summary.loc[len(df_summary)] = [thatCOMID[0], 'low data']\n",
    "\n",
    "        # all_params.append((\"low data\", thatCOMID, median_days2000.shape[0]))\n",
    "        # all_params[thatCOMID] = (\"low data\", median_days2000.shape[0])\n",
    "        continue\n",
    "    \n",
    "    dates = median_days2000.index\n",
    "    widths = median_days2000.values\n",
    "\n",
    "    data = {\n",
    "        'datetime': dates,\n",
    "        'data': widths\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "\n",
    "    try:\n",
    "        groups = [group['data'].values for name, group in df.groupby('month')]\n",
    "        stat, p_value = kruskal(*groups)\n",
    "        \n",
    "        alpha = 0.05\n",
    "        if p_value < alpha:\n",
    "            df_summary.loc[len(df_summary)] = [thatCOMID[0], 'seasonal']\n",
    "        else:\n",
    "            df_summary.loc[len(df_summary)] = [thatCOMID[0], 'non seasonal']\n",
    "\n",
    "    except ValueError as e:\n",
    "        count_lake += 1\n",
    "        df_summary.loc[len(df_summary)] = [thatCOMID[0], 'possible lake']\n",
    "\n",
    "print(count)\n",
    "print('Number of lakes are ', count_lake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b774c02a-aafc-4596-9153-d9d5cc426493",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'NCDF_out/Kruskal_newresult_below_10m_removed.csv'\n",
    "df_summary.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd575191-d9da-4bae-bf19-7695655339de",
   "metadata": {},
   "source": [
    "## Peak month estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32060fa2-d699-41ba-965e-92afe3ec4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct peak month estimation\n",
    "def find_peak_months(df):\n",
    "    peak_months = []\n",
    "        \n",
    "    # Extract year and month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    # Group by year and month, averaging widths for each month\n",
    "    monthly_data = df.groupby(['year', 'month'])['width'].max().reset_index()    ## here 'mean' was used earlier to compute monthly average widths\n",
    "    \n",
    "    # Check if at least 8 months of data are present per year\n",
    "    #valid_years = monthly_data.groupby('year').filter(lambda x: len(x) >= 4)\n",
    "\n",
    "    #if valid_years.shape[0] == 0: return \"never 4 months\"\n",
    "    #if valid_years.shape[0] == 1: return \"just 1 year\"\n",
    "    \n",
    "    # Find the peak month for each year\n",
    "    for year, group in monthly_data.groupby('year'):\n",
    "        peak_month = group.loc[group['width'].idxmax()]['month']\n",
    "        peak_months.append((year, peak_month))\n",
    "\n",
    "    return peak_months\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def filter_distant_months(most_common_months, threshold=1):\n",
    "    \"\"\"\n",
    "    Removes months that are outside the ±threshold range from the most common month.\n",
    "    \"\"\"\n",
    "    top_month, _ = most_common_months[0]\n",
    "    filtered_months = []\n",
    "\n",
    "    # Keep months that are within the threshold proximity of the top month\n",
    "    for month, count in most_common_months:\n",
    "        if abs(top_month - month) <= threshold or abs((top_month + 12) - month) <= threshold:\n",
    "            filtered_months.append((month, count))\n",
    "    \n",
    "    return filtered_months\n",
    "from collections import Counter\n",
    "\n",
    "def determine_peak_month_with_strict_removal(peak_months, proximity_threshold=2):\n",
    "\n",
    "    if peak_months == \"never 8 months\": return  \"never 8 months\"\n",
    "    \n",
    "    # Step 1: Count the frequency of each month\n",
    "    month_counts = Counter([month for year, month in peak_months])\n",
    "    \n",
    "    # Step 2: Sort by frequency\n",
    "    most_common_months = month_counts.most_common()\n",
    "    \n",
    "    # Step 3: Identify the most common (peak) month\n",
    "    if len(most_common_months) > 1:\n",
    "\n",
    "        peak_month = int(np.mean(most_common_months[0]))\n",
    "        \n",
    "        #peak_month = most_common_months[0][0]  # The top peak month\n",
    "        \n",
    "        # Step 4: Ensure all months are within ±2 month of the peak month\n",
    "        #for month, count in most_common_months:\n",
    "        #    if abs(peak_month - month) > proximity_threshold and abs((peak_month + 12) - month) > proximity_threshold:\n",
    "        #        return \"No clear peak\"\n",
    "    else:\n",
    "        peak_month = most_common_months[0][0]\n",
    "\n",
    "    if peak_months == \"just 1 year\": return \"just 1 year \" + str(peak_month)\n",
    "    \n",
    "    return peak_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0178863-a4a2-47b6-9c94-f139faa209de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fb005ade7a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/N/lustre/project/proj-212/abhinav/environment/glows-venv/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/N/lustre/project/proj-212/abhinav/environment/glows-venv/lib/python3.10/site-packages/tqdm/std.py\", line 1267, in close\n",
      "    if self.disable:\n",
      "AttributeError: 'tqdm' object has no attribute 'disable'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 476232/476232 [20:10<00:00, 393.32it/s]\n"
     ]
    }
   ],
   "source": [
    "peak_months_list = []\n",
    "\n",
    "for comid_id, comid_data in tqdm(final_large_data_warm_over60.groupby('COMID')):\n",
    "    peak_months = find_peak_months(comid_data)  # Function assumes list input\n",
    "    peak_month_result = determine_peak_month_with_strict_removal(peak_months)\n",
    "    peak_months_list.append((comid_id, peak_month_result))\n",
    "\n",
    "result_direct_peak_df = pd.DataFrame(peak_months_list, columns=['COMID', 'Direct Peak Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e24e0a3f-760d-4cf2-a0f6-51dd26f438fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476232, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_direct_peak_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa3324fe-7808-45eb-bae8-e9a345601ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMID</th>\n",
       "      <th>Direct Peak Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11000054</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11000302</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11000308</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11000309</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11000766</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476227</th>\n",
       "      <td>86007242</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476228</th>\n",
       "      <td>86007254</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476229</th>\n",
       "      <td>86007258</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476230</th>\n",
       "      <td>86007267</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476231</th>\n",
       "      <td>86007272</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           COMID  Direct Peak Month\n",
       "0       11000054                4.0\n",
       "1       11000302                1.0\n",
       "2       11000308                7.0\n",
       "3       11000309                2.0\n",
       "4       11000766                6.0\n",
       "...          ...                ...\n",
       "476227  86007242                5.0\n",
       "476228  86007254                5.0\n",
       "476229  86007258                4.0\n",
       "476230  86007267                6.0\n",
       "476231  86007272                4.0\n",
       "\n",
       "[476232 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_direct_peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "157e3033-123f-478d-9271-61c78be8ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'NCDF_out/direct_peak newresult_below_10m_removed.csv'\n",
    "result_direct_peak_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca51a3ea-455e-4b61-8e68-264bbff8fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_direct_peak_df_numeric = result_direct_peak_df[\n",
    "    pd.to_numeric(result_direct_peak_df['Direct Peak Month'], errors='coerce').notna()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16314bc-01c6-48f0-b3cd-3a529a1abd19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
